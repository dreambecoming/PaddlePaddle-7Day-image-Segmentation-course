### 1.数据用是 LITS 肝脏数据集28个病例，使用PaddleSeg开发套件进行分割肝脏和肿瘤


```python
#安装 nii处理工具  SimpleITK 和分割工具paddleSeg
!pip install -q SimpleITK paddleseg
```


```python
%cd /home/aistudio/
#首次要运行
!unzip  -o /home/aistudio/data/data79322/traindata.zip -d work/
```

    /home/aistudio
    Archive:  /home/aistudio/data/data79322/traindata.zip
      inflating: work/traindata/data/volume-0.nii  
      inflating: work/traindata/data/volume-1.nii  
      inflating: work/traindata/data/volume-10.nii  
      inflating: work/traindata/data/volume-11.nii  
      inflating: work/traindata/data/volume-12.nii  
      inflating: work/traindata/data/volume-13.nii  
      inflating: work/traindata/data/volume-14.nii  
      inflating: work/traindata/data/volume-15.nii  
      inflating: work/traindata/data/volume-16.nii  
      inflating: work/traindata/data/volume-17.nii  
      inflating: work/traindata/data/volume-18.nii  
      inflating: work/traindata/data/volume-19.nii  
      inflating: work/traindata/data/volume-2.nii  
      inflating: work/traindata/data/volume-20.nii  
      inflating: work/traindata/data/volume-21.nii  
      inflating: work/traindata/data/volume-22.nii  
      inflating: work/traindata/data/volume-23.nii  
      inflating: work/traindata/data/volume-24.nii  
      inflating: work/traindata/data/volume-25.nii  
      inflating: work/traindata/data/volume-26.nii  
      inflating: work/traindata/data/volume-27.nii  
      inflating: work/traindata/data/volume-3.nii  
      inflating: work/traindata/data/volume-4.nii  
      inflating: work/traindata/data/volume-5.nii  
      inflating: work/traindata/data/volume-6.nii  
      inflating: work/traindata/data/volume-7.nii  
      inflating: work/traindata/data/volume-8.nii  
      inflating: work/traindata/data/volume-9.nii  
      inflating: work/traindata/label/segmentation-0.nii  
      inflating: work/traindata/label/segmentation-1.nii  
      inflating: work/traindata/label/segmentation-10.nii  
      inflating: work/traindata/label/segmentation-11.nii  
      inflating: work/traindata/label/segmentation-12.nii  
      inflating: work/traindata/label/segmentation-13.nii  
      inflating: work/traindata/label/segmentation-14.nii  
      inflating: work/traindata/label/segmentation-15.nii  
      inflating: work/traindata/label/segmentation-16.nii  
      inflating: work/traindata/label/segmentation-17.nii  
      inflating: work/traindata/label/segmentation-18.nii  
      inflating: work/traindata/label/segmentation-19.nii  
      inflating: work/traindata/label/segmentation-2.nii  
      inflating: work/traindata/label/segmentation-20.nii  
      inflating: work/traindata/label/segmentation-21.nii  
      inflating: work/traindata/label/segmentation-22.nii  
      inflating: work/traindata/label/segmentation-23.nii  
      inflating: work/traindata/label/segmentation-24.nii  
      inflating: work/traindata/label/segmentation-25.nii  
      inflating: work/traindata/label/segmentation-26.nii  
      inflating: work/traindata/label/segmentation-27.nii  
      inflating: work/traindata/label/segmentation-3.nii  
      inflating: work/traindata/label/segmentation-4.nii  
      inflating: work/traindata/label/segmentation-5.nii  
      inflating: work/traindata/label/segmentation-6.nii  
      inflating: work/traindata/label/segmentation-7.nii  
      inflating: work/traindata/label/segmentation-8.nii  
      inflating: work/traindata/label/segmentation-9.nii  



```python
#导入常用库
import SimpleITK as sitk
import os
import random
import numpy as np
import matplotlib.pyplot as plt
from random import shuffle
import cv2
```

### 2.处理数据，因为数据集里面有些是全腹部的，但是要分割肝脏，所以把不存在肝脏标签的多余‘层’去掉。


```python
raw_dataset_path = 'work/traindata'
# 预处理后的数据集的输出路径
fixed_dataset_path = 'work/new_traindata'
if not os.path.exists(fixed_dataset_path):
    os.mkdir(fixed_dataset_path)
if os.path.exists(fixed_dataset_path):    # 创建保存目录
    os.makedirs(os.path.join(fixed_dataset_path,'data'))
    os.makedirs(os.path.join(fixed_dataset_path,'label'))

upper = 200
lower = -200
for ct_file in os.listdir(os.path.join(raw_dataset_path ,'data')):
    #读取origin
    ct = sitk.ReadImage(os.path.join(os.path.join(raw_dataset_path ,'data'), ct_file), sitk.sitkInt16)
    #转换成 numpy格式
    ct_array = sitk.GetArrayFromImage(ct)

    seg = sitk.ReadImage(os.path.join(os.path.join(raw_dataset_path ,'label'), ct_file.replace('volume', 'segmentation')),
                            sitk.sitkInt8)
    seg_array = sitk.GetArrayFromImage(seg)
    print("裁剪前:{}".format(ct.GetSize(), seg.GetSize()))
    

    # 将灰度值在阈值之外的截断掉
    ct_array[ct_array > upper] = upper
    ct_array[ct_array < lower] = lower

    # 找到肝脏区域开始和结束的slice
    z = np.any(seg_array, axis=(1, 2))
    start_slice, end_slice = np.where(z)[0][[0, -1]]

    ct_array = ct_array[start_slice:end_slice + 1, :, :]
    seg_array = seg_array[start_slice:end_slice + 1, :, :]

    new_ct = sitk.GetImageFromArray(ct_array)
    new_ct.SetDirection(ct.GetDirection())
    new_ct.SetOrigin(ct.GetOrigin())

    new_seg = sitk.GetImageFromArray(seg_array)
    new_seg.SetDirection(ct.GetDirection())
    new_seg.SetOrigin(ct.GetOrigin())
    print("裁剪后:{}".format(new_ct.GetSize(), new_seg.GetSize()))

    sitk.WriteImage(new_ct, os.path.join(os.path.join(fixed_dataset_path ,'data'), ct_file))
    sitk.WriteImage(new_seg,
                    os.path.join(os.path.join(fixed_dataset_path , 'label'), ct_file.replace('volume', 'segmentation')))
```

    裁剪前:(512, 512, 668)
    裁剪后:(512, 512, 192)
    裁剪前:(512, 512, 574)
    裁剪后:(512, 512, 194)
    裁剪前:(512, 512, 541)
    裁剪后:(512, 512, 179)
    裁剪前:(512, 512, 861)
    裁剪后:(512, 512, 229)
    裁剪前:(512, 512, 565)
    裁剪后:(512, 512, 133)
    裁剪前:(512, 512, 391)
    裁剪后:(512, 512, 116)
    裁剪前:(512, 512, 455)
    裁剪后:(512, 512, 189)
    裁剪前:(512, 512, 689)
    裁剪后:(512, 512, 187)
    裁剪前:(512, 512, 541)
    裁剪后:(512, 512, 177)
    裁剪前:(512, 512, 601)
    裁剪后:(512, 512, 232)
    裁剪前:(512, 512, 826)
    裁剪后:(512, 512, 198)
    裁剪前:(512, 512, 517)
    裁剪后:(512, 512, 139)
    裁剪前:(512, 512, 518)
    裁剪后:(512, 512, 186)
    裁剪前:(512, 512, 75)
    裁剪后:(512, 512, 29)
    裁剪前:(512, 512, 605)
    裁剪后:(512, 512, 142)
    裁剪前:(512, 512, 547)
    裁剪后:(512, 512, 188)
    裁剪前:(512, 512, 537)
    裁剪后:(512, 512, 176)
    裁剪前:(512, 512, 437)
    裁剪后:(512, 512, 175)
    裁剪前:(512, 512, 549)
    裁剪后:(512, 512, 173)
    裁剪前:(512, 512, 588)
    裁剪后:(512, 512, 139)
    裁剪前:(512, 512, 466)
    裁剪后:(512, 512, 167)
    裁剪前:(512, 512, 123)
    裁剪后:(512, 512, 29)
    裁剪前:(512, 512, 845)
    裁剪后:(512, 512, 189)
    裁剪前:(512, 512, 247)
    裁剪后:(512, 512, 46)
    裁剪前:(512, 512, 534)
    裁剪后:(512, 512, 169)
    裁剪前:(512, 512, 276)
    裁剪后:(512, 512, 118)
    裁剪前:(512, 512, 501)
    裁剪后:(512, 512, 181)
    裁剪前:(512, 512, 841)
    裁剪后:(512, 512, 250)


### 3. 把nii的数据保存为jpg格式


```python
data_path = 'work/new_traindata/data'
label_path = 'work/new_traindata/label'
count = 0
if not os.path.exists('/home/aistudio/work/newdata'):
    os.mkdir('/home/aistudio/work/newdata')
    os.makedirs(os.path.join('/home/aistudio/work/newdata','origin'))
    os.makedirs(os.path.join('/home/aistudio/work/newdata','label'))
for f in os.listdir(data_path):
    origin_path= os.path.join(data_path, f)
    seg_path = os.path.join(label_path,f).replace('volume','segmentation')
    origin_array = sitk.GetArrayFromImage(sitk.ReadImage(origin_path))
    seg_array = sitk.GetArrayFromImage(sitk.ReadImage(seg_path))
    for i in range(seg_array.shape[0]):
        seg_image = seg_array[i,:,:]
        seg_image = np.rot90(np.transpose(seg_image, (1,0)))
        origin_image = origin_array[i,:,:]
        origin_image = np.rot90(np.transpose(origin_image, (1,0)))
        cv2.imwrite('work/newdata/label/'+str(count) + '.png', seg_image)
        cv2.imwrite('work/newdata/origin/'+str(count) + '.jpg', origin_image)
        count += 1

print(count)
```

    4522



```python
image = cv2.imread('work/newdata/origin/51.jpg',0)
label = cv2.imread('work/newdata/label/51.png',0)
plt.figure(figsize=(10,5))
plt.subplot(121)
plt.imshow(image,'gray')

plt.subplot(122)
plt.imshow(label, 'gray')
plt.show()
```


![png](output_8_0.png)


### 4.创建 train.txt, val.txt, test.txt文档


```python
random.seed(2021)
path_origin = '/home/aistudio/work/newdata/origin'
path_label = '/home/aistudio/work/newdata/label'
files = list(filter(lambda x: x.endswith('.jpg'), os.listdir(path_origin)))
random.shuffle(files)
rate = int(len(files) * 0.8)#训练集和测试集8：2
train_txt = open('/home/aistudio/work/newdata/train_list.txt','w')
val_txt = open('/home/aistudio/work/newdata/val_list.txt','w')
test_txt = open('/home/aistudio/work/newdata/test_list.txt','w')
for i,f in enumerate(files):
    image_path = os.path.join(path_origin, f)
    label_name = f.split('.')[0]+ '.png'
    label_path = os.path.join(path_label, label_name)
    if i < rate:
        train_txt.write(image_path + ' ' + label_path+ '\n')
    else:
        if i%2 :
            val_txt.write(image_path + ' ' + label_path+ '\n')
        else:
            test_txt.write(image_path + ' ' + label_path+ '\n')
train_txt.close()
val_txt.close()
test_txt.close()
print('完成')
```

    完成


### 5.创建Transform  和DataSet


```python
import paddleseg.transforms as T
from paddleseg.datasets import OpticDiscSeg,Dataset

train_transforms = [
    # T.RandomHorizontalFlip(),                                                              # 水平翻转
    # T.RandomDistort(),                                                                     # 随机扭曲
    # T.RandomRotation(max_rotation = 10,im_padding_value =(0,0,0),label_padding_value = 0), # 随机旋转
    # T.RandomBlur(),                                                                        # 随机模糊
    # T.RandomScaleAspect(min_scale = 0.8, aspect_ratio = 0.5),                              # 随机缩放
    T.Resize(target_size=(512, 512)),
    T.Normalize()                                                                          # 归一化 mean Default: [0.5, 0.5, 0.5]  std Default: [0.5, 0.5, 0.5].
]
val_transforms = [
    #补全
    T.Resize(target_size=(512,512)),
    T.Normalize() 
]
test_transforms = [
    T.Resize(target_size=(512,512)),
    T.Normalize()
]

dataset_root = '/home/aistudio/work/newdata'
train_path  = '/home/aistudio/work/newdata/train_list.txt'
val_path  = '/home/aistudio/work/newdata/val_list.txt'
test_path  = '/home/aistudio/work/newdata/test_list.txt'

# 构建训练集
train_dataset = Dataset(
    #补全
    dataset_root = dataset_root,
    train_path = train_path,
    transforms = train_transforms,
    num_classes = 3,
    mode = 'train'
                  )
#验证集
val_dataset = Dataset(
    #补全
    dataset_root =dataset_root,
    val_path = val_path,
    num_classes = 3,
    transforms = val_transforms,
    mode = 'val'
                  )

#测试集
test_dataset = Dataset(
    #补全
    dataset_root = dataset_root,
    test_path = test_path,
    num_classes = 3,
    transforms = test_transforms,
    mode = 'test'
                  
                  )
```


```python
#预览数据
#没有显示 ，在运行一次
import matplotlib.pyplot as plt
import numpy as np
plt.figure(figsize=(16,16))
for i in range(1,6,2):
    img, label = train_dataset[131]
    img = np.transpose(img, (1,2,0))
    img = img*0.5 + 0.5
    plt.subplot(3,2,i),plt.imshow(img,'gray'),plt.title('img'),plt.xticks([]),plt.yticks([])
    plt.subplot(3,2,i+1),plt.imshow(label,'gray'),plt.title('label'),plt.xticks([]),plt.yticks([])
    plt.show
```


![png](output_13_0.png)


### 6.设置网络和 损失函数、优化器等


```python
from paddleseg.models import UNet
model = UNet(num_classes=3)
```


```python
from paddleseg.models.losses import CrossEntropyLoss,DiceLoss
import paddle
# 设置学习率,图像分类课程中学的最优学习率
base_lr = 3e-4
#自己换学习率和优化器，能不能上高分
#配上Adam优化器
lr = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=base_lr, T_max=1800, verbose=False)
optimizer = paddle.optimizer.Adam(learning_rate=3e-4, parameters=model.parameters())
losses = {}
#自己尝试组合dice损失函数，会不会效果更好
losses['types'] = [CrossEntropyLoss()]
losses['coef'] = [1]
```

### 7.开始训练


```python
from paddleseg.core import train

train(
    model=model,
    train_dataset=train_dataset,       # 填写训练集的dataset
    val_dataset=val_dataset,           # 填写验证集的dataset
    optimizer=optimizer,               # 优化器
    save_dir='save_model',    # 保存路径
    iters=2000,                        # 训练次数
    batch_size=4,                      # 每批处理图片的张数
    save_interval=200,                 # 保存的间隔次数
    log_iters=10,                      # 日志打印间隔
    num_workers=0,                     # 异步加载数据的进程数目
    losses=losses,                     # 传入loss函数
    use_vdl=True)                      # 是否使用visualDL
```

    2021-04-15 23:23:24 [INFO]	[TRAIN] epoch=1, iter=10/2000, loss=0.5670, lr=0.000300, batch_cost=0.2710, reader_cost=0.01286, ips=14.7620 samples/sec | ETA 00:08:59
    2021-04-15 23:23:26 [INFO]	[TRAIN] epoch=1, iter=20/2000, loss=0.1900, lr=0.000300, batch_cost=0.2549, reader_cost=0.00015, ips=15.6944 samples/sec | ETA 00:08:24
    2021-04-15 23:23:29 [INFO]	[TRAIN] epoch=1, iter=30/2000, loss=0.1069, lr=0.000300, batch_cost=0.2570, reader_cost=0.00040, ips=15.5634 samples/sec | ETA 00:08:26
    2021-04-15 23:23:31 [INFO]	[TRAIN] epoch=1, iter=40/2000, loss=0.0767, lr=0.000300, batch_cost=0.2534, reader_cost=0.00011, ips=15.7863 samples/sec | ETA 00:08:16
    2021-04-15 23:23:34 [INFO]	[TRAIN] epoch=1, iter=50/2000, loss=0.0810, lr=0.000300, batch_cost=0.2527, reader_cost=0.00010, ips=15.8301 samples/sec | ETA 00:08:12
    2021-04-15 23:23:36 [INFO]	[TRAIN] epoch=1, iter=60/2000, loss=0.0616, lr=0.000300, batch_cost=0.2530, reader_cost=0.00011, ips=15.8133 samples/sec | ETA 00:08:10
    2021-04-15 23:23:39 [INFO]	[TRAIN] epoch=1, iter=70/2000, loss=0.0643, lr=0.000300, batch_cost=0.2529, reader_cost=0.00010, ips=15.8173 samples/sec | ETA 00:08:08
    2021-04-15 23:23:42 [INFO]	[TRAIN] epoch=1, iter=80/2000, loss=0.0476, lr=0.000300, batch_cost=0.2531, reader_cost=0.00011, ips=15.8027 samples/sec | ETA 00:08:05
    2021-04-15 23:23:44 [INFO]	[TRAIN] epoch=1, iter=90/2000, loss=0.0491, lr=0.000300, batch_cost=0.2529, reader_cost=0.00011, ips=15.8196 samples/sec | ETA 00:08:02
    2021-04-15 23:23:47 [INFO]	[TRAIN] epoch=1, iter=100/2000, loss=0.0528, lr=0.000300, batch_cost=0.2533, reader_cost=0.00009, ips=15.7887 samples/sec | ETA 00:08:01
    2021-04-15 23:23:49 [INFO]	[TRAIN] epoch=1, iter=110/2000, loss=0.0537, lr=0.000300, batch_cost=0.2535, reader_cost=0.00010, ips=15.7816 samples/sec | ETA 00:07:59
    2021-04-15 23:23:52 [INFO]	[TRAIN] epoch=1, iter=120/2000, loss=0.0578, lr=0.000300, batch_cost=0.2535, reader_cost=0.00010, ips=15.7765 samples/sec | ETA 00:07:56
    2021-04-15 23:23:54 [INFO]	[TRAIN] epoch=1, iter=130/2000, loss=0.0403, lr=0.000300, batch_cost=0.2531, reader_cost=0.00010, ips=15.8013 samples/sec | ETA 00:07:53
    2021-04-15 23:23:57 [INFO]	[TRAIN] epoch=1, iter=140/2000, loss=0.0438, lr=0.000300, batch_cost=0.2531, reader_cost=0.00010, ips=15.8064 samples/sec | ETA 00:07:50
    2021-04-15 23:23:59 [INFO]	[TRAIN] epoch=1, iter=150/2000, loss=0.0575, lr=0.000300, batch_cost=0.2526, reader_cost=0.00009, ips=15.8351 samples/sec | ETA 00:07:47
    2021-04-15 23:24:02 [INFO]	[TRAIN] epoch=1, iter=160/2000, loss=0.0427, lr=0.000300, batch_cost=0.2534, reader_cost=0.00012, ips=15.7835 samples/sec | ETA 00:07:46
    2021-04-15 23:24:04 [INFO]	[TRAIN] epoch=1, iter=170/2000, loss=0.0353, lr=0.000300, batch_cost=0.2541, reader_cost=0.00011, ips=15.7414 samples/sec | ETA 00:07:45
    2021-04-15 23:24:07 [INFO]	[TRAIN] epoch=1, iter=180/2000, loss=0.0413, lr=0.000300, batch_cost=0.2535, reader_cost=0.00013, ips=15.7814 samples/sec | ETA 00:07:41
    2021-04-15 23:24:09 [INFO]	[TRAIN] epoch=1, iter=190/2000, loss=0.0349, lr=0.000300, batch_cost=0.2544, reader_cost=0.00014, ips=15.7219 samples/sec | ETA 00:07:40
    2021-04-15 23:24:12 [INFO]	[TRAIN] epoch=1, iter=200/2000, loss=0.0396, lr=0.000300, batch_cost=0.2538, reader_cost=0.00011, ips=15.7607 samples/sec | ETA 00:07:36
    2021-04-15 23:24:12 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0171 - reader cost: 1.9212e-


    2021-04-15 23:24:20 [INFO]	[EVAL] #Images=453 mIoU=0.5618 Acc=0.9698 Kappa=0.7945 
    2021-04-15 23:24:20 [INFO]	[EVAL] Class IoU: 
    [0.9697 0.6783 0.0373]
    2021-04-15 23:24:20 [INFO]	[EVAL] Class Acc: 
    [0.9983 0.6892 0.4795]
    2021-04-15 23:24:22 [INFO]	[EVAL] The model with the best validation mIoU (0.5618) was saved at iter 200.
    2021-04-15 23:24:25 [INFO]	[TRAIN] epoch=1, iter=210/2000, loss=0.0322, lr=0.000300, batch_cost=0.2530, reader_cost=0.00011, ips=15.8129 samples/sec | ETA 00:07:32
    2021-04-15 23:24:27 [INFO]	[TRAIN] epoch=1, iter=220/2000, loss=0.0338, lr=0.000300, batch_cost=0.2534, reader_cost=0.00012, ips=15.7871 samples/sec | ETA 00:07:31
    2021-04-15 23:24:30 [INFO]	[TRAIN] epoch=1, iter=230/2000, loss=0.0385, lr=0.000300, batch_cost=0.2537, reader_cost=0.00011, ips=15.7665 samples/sec | ETA 00:07:29
    2021-04-15 23:24:32 [INFO]	[TRAIN] epoch=1, iter=240/2000, loss=0.0453, lr=0.000300, batch_cost=0.2537, reader_cost=0.00011, ips=15.7684 samples/sec | ETA 00:07:26
    2021-04-15 23:24:35 [INFO]	[TRAIN] epoch=1, iter=250/2000, loss=0.0356, lr=0.000300, batch_cost=0.2528, reader_cost=0.00010, ips=15.8242 samples/sec | ETA 00:07:22
    2021-04-15 23:24:37 [INFO]	[TRAIN] epoch=1, iter=260/2000, loss=0.0306, lr=0.000300, batch_cost=0.2533, reader_cost=0.00011, ips=15.7890 samples/sec | ETA 00:07:20
    2021-04-15 23:24:40 [INFO]	[TRAIN] epoch=1, iter=270/2000, loss=0.0265, lr=0.000300, batch_cost=0.2530, reader_cost=0.00014, ips=15.8127 samples/sec | ETA 00:07:17
    2021-04-15 23:24:42 [INFO]	[TRAIN] epoch=1, iter=280/2000, loss=0.0318, lr=0.000300, batch_cost=0.2540, reader_cost=0.00010, ips=15.7456 samples/sec | ETA 00:07:16
    2021-04-15 23:24:45 [INFO]	[TRAIN] epoch=1, iter=290/2000, loss=0.0261, lr=0.000300, batch_cost=0.2538, reader_cost=0.00011, ips=15.7607 samples/sec | ETA 00:07:13
    2021-04-15 23:24:47 [INFO]	[TRAIN] epoch=1, iter=300/2000, loss=0.0241, lr=0.000300, batch_cost=0.2537, reader_cost=0.00010, ips=15.7667 samples/sec | ETA 00:07:11
    2021-04-15 23:24:50 [INFO]	[TRAIN] epoch=1, iter=310/2000, loss=0.0345, lr=0.000300, batch_cost=0.2532, reader_cost=0.00015, ips=15.7948 samples/sec | ETA 00:07:07
    2021-04-15 23:24:53 [INFO]	[TRAIN] epoch=1, iter=320/2000, loss=0.0272, lr=0.000300, batch_cost=0.2539, reader_cost=0.00010, ips=15.7534 samples/sec | ETA 00:07:06
    2021-04-15 23:24:55 [INFO]	[TRAIN] epoch=1, iter=330/2000, loss=0.0273, lr=0.000300, batch_cost=0.2550, reader_cost=0.00010, ips=15.6882 samples/sec | ETA 00:07:05
    2021-04-15 23:24:58 [INFO]	[TRAIN] epoch=1, iter=340/2000, loss=0.0288, lr=0.000300, batch_cost=0.2538, reader_cost=0.00010, ips=15.7596 samples/sec | ETA 00:07:01
    2021-04-15 23:25:00 [INFO]	[TRAIN] epoch=1, iter=350/2000, loss=0.0265, lr=0.000300, batch_cost=0.2531, reader_cost=0.00011, ips=15.8053 samples/sec | ETA 00:06:57
    2021-04-15 23:25:03 [INFO]	[TRAIN] epoch=1, iter=360/2000, loss=0.0281, lr=0.000300, batch_cost=0.2537, reader_cost=0.00010, ips=15.7637 samples/sec | ETA 00:06:56
    2021-04-15 23:25:05 [INFO]	[TRAIN] epoch=1, iter=370/2000, loss=0.0255, lr=0.000300, batch_cost=0.2540, reader_cost=0.00011, ips=15.7511 samples/sec | ETA 00:06:53
    2021-04-15 23:25:08 [INFO]	[TRAIN] epoch=1, iter=380/2000, loss=0.0216, lr=0.000300, batch_cost=0.2545, reader_cost=0.00012, ips=15.7147 samples/sec | ETA 00:06:52
    2021-04-15 23:25:10 [INFO]	[TRAIN] epoch=1, iter=390/2000, loss=0.0289, lr=0.000300, batch_cost=0.2543, reader_cost=0.00012, ips=15.7310 samples/sec | ETA 00:06:49
    2021-04-15 23:25:13 [INFO]	[TRAIN] epoch=1, iter=400/2000, loss=0.0428, lr=0.000300, batch_cost=0.2535, reader_cost=0.00009, ips=15.7787 samples/sec | ETA 00:06:45
    2021-04-15 23:25:13 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 17ms/step - batch_cost: 0.0171 - reader cost: 1.2650e-


    2021-04-15 23:25:21 [INFO]	[EVAL] #Images=453 mIoU=0.7367 Acc=0.9908 Kappa=0.9265 
    2021-04-15 23:25:21 [INFO]	[EVAL] Class IoU: 
    [0.9924 0.872  0.3458]
    2021-04-15 23:25:21 [INFO]	[EVAL] Class Acc: 
    [0.9958 0.9443 0.433 ]
    2021-04-15 23:25:23 [INFO]	[EVAL] The model with the best validation mIoU (0.7367) was saved at iter 400.
    2021-04-15 23:25:25 [INFO]	[TRAIN] epoch=1, iter=410/2000, loss=0.0234, lr=0.000300, batch_cost=0.2535, reader_cost=0.00011, ips=15.7769 samples/sec | ETA 00:06:43
    2021-04-15 23:25:28 [INFO]	[TRAIN] epoch=1, iter=420/2000, loss=0.0256, lr=0.000300, batch_cost=0.2534, reader_cost=0.00010, ips=15.7878 samples/sec | ETA 00:06:40
    2021-04-15 23:25:31 [INFO]	[TRAIN] epoch=1, iter=430/2000, loss=0.0251, lr=0.000300, batch_cost=0.2549, reader_cost=0.00017, ips=15.6947 samples/sec | ETA 00:06:40
    2021-04-15 23:25:33 [INFO]	[TRAIN] epoch=1, iter=440/2000, loss=0.0248, lr=0.000300, batch_cost=0.2537, reader_cost=0.00015, ips=15.7642 samples/sec | ETA 00:06:35
    2021-04-15 23:25:36 [INFO]	[TRAIN] epoch=1, iter=450/2000, loss=0.0316, lr=0.000300, batch_cost=0.2553, reader_cost=0.00011, ips=15.6706 samples/sec | ETA 00:06:35
    2021-04-15 23:25:38 [INFO]	[TRAIN] epoch=1, iter=460/2000, loss=0.0252, lr=0.000300, batch_cost=0.2541, reader_cost=0.00010, ips=15.7397 samples/sec | ETA 00:06:31
    2021-04-15 23:25:41 [INFO]	[TRAIN] epoch=1, iter=470/2000, loss=0.0177, lr=0.000300, batch_cost=0.2546, reader_cost=0.00011, ips=15.7088 samples/sec | ETA 00:06:29
    2021-04-15 23:25:43 [INFO]	[TRAIN] epoch=1, iter=480/2000, loss=0.0198, lr=0.000300, batch_cost=0.2594, reader_cost=0.00010, ips=15.4192 samples/sec | ETA 00:06:34
    2021-04-15 23:25:46 [INFO]	[TRAIN] epoch=1, iter=490/2000, loss=0.0202, lr=0.000300, batch_cost=0.2545, reader_cost=0.00010, ips=15.7144 samples/sec | ETA 00:06:24
    2021-04-15 23:25:48 [INFO]	[TRAIN] epoch=1, iter=500/2000, loss=0.0200, lr=0.000300, batch_cost=0.2542, reader_cost=0.00010, ips=15.7387 samples/sec | ETA 00:06:21
    2021-04-15 23:25:51 [INFO]	[TRAIN] epoch=1, iter=510/2000, loss=0.0245, lr=0.000300, batch_cost=0.2534, reader_cost=0.00011, ips=15.7823 samples/sec | ETA 00:06:17
    2021-04-15 23:25:54 [INFO]	[TRAIN] epoch=1, iter=520/2000, loss=0.0196, lr=0.000300, batch_cost=0.2541, reader_cost=0.00015, ips=15.7432 samples/sec | ETA 00:06:16
    2021-04-15 23:25:56 [INFO]	[TRAIN] epoch=1, iter=530/2000, loss=0.0323, lr=0.000300, batch_cost=0.2536, reader_cost=0.00013, ips=15.7741 samples/sec | ETA 00:06:12
    2021-04-15 23:25:59 [INFO]	[TRAIN] epoch=1, iter=540/2000, loss=0.0263, lr=0.000300, batch_cost=0.2533, reader_cost=0.00013, ips=15.7910 samples/sec | ETA 00:06:09
    2021-04-15 23:26:01 [INFO]	[TRAIN] epoch=1, iter=550/2000, loss=0.0218, lr=0.000300, batch_cost=0.2551, reader_cost=0.00012, ips=15.6787 samples/sec | ETA 00:06:09
    2021-04-15 23:26:04 [INFO]	[TRAIN] epoch=1, iter=560/2000, loss=0.0163, lr=0.000300, batch_cost=0.2546, reader_cost=0.00012, ips=15.7138 samples/sec | ETA 00:06:06
    2021-04-15 23:26:06 [INFO]	[TRAIN] epoch=1, iter=570/2000, loss=0.0244, lr=0.000300, batch_cost=0.2549, reader_cost=0.00011, ips=15.6910 samples/sec | ETA 00:06:04
    2021-04-15 23:26:09 [INFO]	[TRAIN] epoch=1, iter=580/2000, loss=0.0228, lr=0.000300, batch_cost=0.2538, reader_cost=0.00014, ips=15.7597 samples/sec | ETA 00:06:00
    2021-04-15 23:26:11 [INFO]	[TRAIN] epoch=1, iter=590/2000, loss=0.0254, lr=0.000300, batch_cost=0.2544, reader_cost=0.00010, ips=15.7233 samples/sec | ETA 00:05:58
    2021-04-15 23:26:14 [INFO]	[TRAIN] epoch=1, iter=600/2000, loss=0.0250, lr=0.000300, batch_cost=0.2542, reader_cost=0.00011, ips=15.7376 samples/sec | ETA 00:05:55
    2021-04-15 23:26:14 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0172 - reader cost: 1.2956e-


    2021-04-15 23:26:22 [INFO]	[EVAL] #Images=453 mIoU=0.7608 Acc=0.9912 Kappa=0.9272 
    2021-04-15 23:26:22 [INFO]	[EVAL] Class IoU: 
    [0.9917 0.8714 0.4193]
    2021-04-15 23:26:22 [INFO]	[EVAL] Class Acc: 
    [0.9935 0.9559 0.936 ]
    2021-04-15 23:26:24 [INFO]	[EVAL] The model with the best validation mIoU (0.7608) was saved at iter 600.
    2021-04-15 23:26:27 [INFO]	[TRAIN] epoch=1, iter=610/2000, loss=0.0233, lr=0.000300, batch_cost=0.2528, reader_cost=0.00011, ips=15.8246 samples/sec | ETA 00:05:51
    2021-04-15 23:26:29 [INFO]	[TRAIN] epoch=1, iter=620/2000, loss=0.0214, lr=0.000300, batch_cost=0.2539, reader_cost=0.00011, ips=15.7561 samples/sec | ETA 00:05:50
    2021-04-15 23:26:32 [INFO]	[TRAIN] epoch=1, iter=630/2000, loss=0.0191, lr=0.000300, batch_cost=0.2538, reader_cost=0.00011, ips=15.7589 samples/sec | ETA 00:05:47
    2021-04-15 23:26:34 [INFO]	[TRAIN] epoch=1, iter=640/2000, loss=0.0190, lr=0.000300, batch_cost=0.2550, reader_cost=0.00012, ips=15.6839 samples/sec | ETA 00:05:46
    2021-04-15 23:26:37 [INFO]	[TRAIN] epoch=1, iter=650/2000, loss=0.0196, lr=0.000300, batch_cost=0.2535, reader_cost=0.00011, ips=15.7787 samples/sec | ETA 00:05:42
    2021-04-15 23:26:39 [INFO]	[TRAIN] epoch=1, iter=660/2000, loss=0.0178, lr=0.000300, batch_cost=0.2544, reader_cost=0.00015, ips=15.7251 samples/sec | ETA 00:05:40
    2021-04-15 23:26:42 [INFO]	[TRAIN] epoch=1, iter=670/2000, loss=0.0154, lr=0.000300, batch_cost=0.2554, reader_cost=0.00013, ips=15.6599 samples/sec | ETA 00:05:39
    2021-04-15 23:26:44 [INFO]	[TRAIN] epoch=1, iter=680/2000, loss=0.0169, lr=0.000300, batch_cost=0.2543, reader_cost=0.00010, ips=15.7287 samples/sec | ETA 00:05:35
    2021-04-15 23:26:47 [INFO]	[TRAIN] epoch=1, iter=690/2000, loss=0.0176, lr=0.000300, batch_cost=0.2549, reader_cost=0.00011, ips=15.6901 samples/sec | ETA 00:05:33
    2021-04-15 23:26:50 [INFO]	[TRAIN] epoch=1, iter=700/2000, loss=0.0141, lr=0.000300, batch_cost=0.2545, reader_cost=0.00011, ips=15.7198 samples/sec | ETA 00:05:30
    2021-04-15 23:26:52 [INFO]	[TRAIN] epoch=1, iter=710/2000, loss=0.0168, lr=0.000300, batch_cost=0.2540, reader_cost=0.00013, ips=15.7486 samples/sec | ETA 00:05:27
    2021-04-15 23:26:55 [INFO]	[TRAIN] epoch=1, iter=720/2000, loss=0.0175, lr=0.000300, batch_cost=0.2548, reader_cost=0.00011, ips=15.6979 samples/sec | ETA 00:05:26
    2021-04-15 23:26:57 [INFO]	[TRAIN] epoch=1, iter=730/2000, loss=0.0248, lr=0.000300, batch_cost=0.2537, reader_cost=0.00013, ips=15.7654 samples/sec | ETA 00:05:22
    2021-04-15 23:27:00 [INFO]	[TRAIN] epoch=1, iter=740/2000, loss=0.0188, lr=0.000300, batch_cost=0.2543, reader_cost=0.00013, ips=15.7320 samples/sec | ETA 00:05:20
    2021-04-15 23:27:02 [INFO]	[TRAIN] epoch=1, iter=750/2000, loss=0.0185, lr=0.000300, batch_cost=0.2543, reader_cost=0.00010, ips=15.7291 samples/sec | ETA 00:05:17
    2021-04-15 23:27:05 [INFO]	[TRAIN] epoch=1, iter=760/2000, loss=0.0192, lr=0.000300, batch_cost=0.2536, reader_cost=0.00011, ips=15.7720 samples/sec | ETA 00:05:14
    2021-04-15 23:27:07 [INFO]	[TRAIN] epoch=1, iter=770/2000, loss=0.0206, lr=0.000300, batch_cost=0.2536, reader_cost=0.00011, ips=15.7744 samples/sec | ETA 00:05:11
    2021-04-15 23:27:10 [INFO]	[TRAIN] epoch=1, iter=780/2000, loss=0.0165, lr=0.000300, batch_cost=0.2545, reader_cost=0.00015, ips=15.7141 samples/sec | ETA 00:05:10
    2021-04-15 23:27:12 [INFO]	[TRAIN] epoch=1, iter=790/2000, loss=0.0171, lr=0.000300, batch_cost=0.2547, reader_cost=0.00011, ips=15.7067 samples/sec | ETA 00:05:08
    2021-04-15 23:27:15 [INFO]	[TRAIN] epoch=1, iter=800/2000, loss=0.0194, lr=0.000300, batch_cost=0.2538, reader_cost=0.00016, ips=15.7588 samples/sec | ETA 00:05:04
    2021-04-15 23:27:15 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0171 - reader cost: 1.2300e-


    2021-04-15 23:27:23 [INFO]	[EVAL] #Images=453 mIoU=0.8115 Acc=0.9937 Kappa=0.9498 
    2021-04-15 23:27:23 [INFO]	[EVAL] Class IoU: 
    [0.9945 0.9105 0.5296]
    2021-04-15 23:27:23 [INFO]	[EVAL] Class Acc: 
    [0.9969 0.959  0.6626]
    2021-04-15 23:27:25 [INFO]	[EVAL] The model with the best validation mIoU (0.8115) was saved at iter 800.
    2021-04-15 23:27:28 [INFO]	[TRAIN] epoch=1, iter=810/2000, loss=0.0183, lr=0.000300, batch_cost=0.2574, reader_cost=0.00014, ips=15.5423 samples/sec | ETA 00:05:06
    2021-04-15 23:27:30 [INFO]	[TRAIN] epoch=1, iter=820/2000, loss=0.0167, lr=0.000300, batch_cost=0.2566, reader_cost=0.00014, ips=15.5907 samples/sec | ETA 00:05:02
    2021-04-15 23:27:33 [INFO]	[TRAIN] epoch=1, iter=830/2000, loss=0.0181, lr=0.000300, batch_cost=0.2535, reader_cost=0.00010, ips=15.7805 samples/sec | ETA 00:04:56
    2021-04-15 23:27:35 [INFO]	[TRAIN] epoch=1, iter=840/2000, loss=0.0174, lr=0.000300, batch_cost=0.2544, reader_cost=0.00010, ips=15.7209 samples/sec | ETA 00:04:55
    2021-04-15 23:27:38 [INFO]	[TRAIN] epoch=1, iter=850/2000, loss=0.0214, lr=0.000300, batch_cost=0.2542, reader_cost=0.00012, ips=15.7366 samples/sec | ETA 00:04:52
    2021-04-15 23:27:41 [INFO]	[TRAIN] epoch=1, iter=860/2000, loss=0.0311, lr=0.000300, batch_cost=0.2542, reader_cost=0.00011, ips=15.7360 samples/sec | ETA 00:04:49
    2021-04-15 23:27:43 [INFO]	[TRAIN] epoch=1, iter=870/2000, loss=0.0189, lr=0.000300, batch_cost=0.2549, reader_cost=0.00011, ips=15.6952 samples/sec | ETA 00:04:47
    2021-04-15 23:27:46 [INFO]	[TRAIN] epoch=1, iter=880/2000, loss=0.0206, lr=0.000300, batch_cost=0.2536, reader_cost=0.00013, ips=15.7710 samples/sec | ETA 00:04:44
    2021-04-15 23:27:48 [INFO]	[TRAIN] epoch=1, iter=890/2000, loss=0.0231, lr=0.000300, batch_cost=0.2551, reader_cost=0.00010, ips=15.6815 samples/sec | ETA 00:04:43
    2021-04-15 23:27:51 [INFO]	[TRAIN] epoch=1, iter=900/2000, loss=0.0202, lr=0.000300, batch_cost=0.2547, reader_cost=0.00012, ips=15.7046 samples/sec | ETA 00:04:40
    2021-04-15 23:27:53 [INFO]	[TRAIN] epoch=2, iter=910/2000, loss=0.0204, lr=0.000300, batch_cost=0.2621, reader_cost=0.00707, ips=15.2638 samples/sec | ETA 00:04:45
    2021-04-15 23:27:56 [INFO]	[TRAIN] epoch=2, iter=920/2000, loss=0.0176, lr=0.000300, batch_cost=0.2560, reader_cost=0.00011, ips=15.6222 samples/sec | ETA 00:04:36
    2021-04-15 23:27:58 [INFO]	[TRAIN] epoch=2, iter=930/2000, loss=0.0243, lr=0.000300, batch_cost=0.2584, reader_cost=0.00010, ips=15.4777 samples/sec | ETA 00:04:36
    2021-04-15 23:28:01 [INFO]	[TRAIN] epoch=2, iter=940/2000, loss=0.0192, lr=0.000300, batch_cost=0.2540, reader_cost=0.00011, ips=15.7453 samples/sec | ETA 00:04:29
    2021-04-15 23:28:04 [INFO]	[TRAIN] epoch=2, iter=950/2000, loss=0.0165, lr=0.000300, batch_cost=0.2579, reader_cost=0.00011, ips=15.5115 samples/sec | ETA 00:04:30
    2021-04-15 23:28:06 [INFO]	[TRAIN] epoch=2, iter=960/2000, loss=0.0209, lr=0.000300, batch_cost=0.2596, reader_cost=0.00011, ips=15.4083 samples/sec | ETA 00:04:29
    2021-04-15 23:28:09 [INFO]	[TRAIN] epoch=2, iter=970/2000, loss=0.0172, lr=0.000300, batch_cost=0.2556, reader_cost=0.00013, ips=15.6512 samples/sec | ETA 00:04:23
    2021-04-15 23:28:11 [INFO]	[TRAIN] epoch=2, iter=980/2000, loss=0.0223, lr=0.000300, batch_cost=0.2542, reader_cost=0.00011, ips=15.7379 samples/sec | ETA 00:04:19
    2021-04-15 23:28:14 [INFO]	[TRAIN] epoch=2, iter=990/2000, loss=0.0196, lr=0.000300, batch_cost=0.2541, reader_cost=0.00015, ips=15.7445 samples/sec | ETA 00:04:16
    2021-04-15 23:28:16 [INFO]	[TRAIN] epoch=2, iter=1000/2000, loss=0.0166, lr=0.000300, batch_cost=0.2542, reader_cost=0.00017, ips=15.7332 samples/sec | ETA 00:04:14
    2021-04-15 23:28:16 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0172 - reader cost: 1.2756e-0


    2021-04-15 23:28:24 [INFO]	[EVAL] #Images=453 mIoU=0.8172 Acc=0.9941 Kappa=0.9526 
    2021-04-15 23:28:24 [INFO]	[EVAL] Class IoU: 
    [0.9944 0.9172 0.5399]
    2021-04-15 23:28:24 [INFO]	[EVAL] Class Acc: 
    [0.9972 0.9523 0.8808]
    2021-04-15 23:28:27 [INFO]	[EVAL] The model with the best validation mIoU (0.8172) was saved at iter 1000.
    2021-04-15 23:28:29 [INFO]	[TRAIN] epoch=2, iter=1010/2000, loss=0.0168, lr=0.000300, batch_cost=0.2538, reader_cost=0.00017, ips=15.7585 samples/sec | ETA 00:04:11
    2021-04-15 23:28:32 [INFO]	[TRAIN] epoch=2, iter=1020/2000, loss=0.0147, lr=0.000300, batch_cost=0.2533, reader_cost=0.00014, ips=15.7891 samples/sec | ETA 00:04:08
    2021-04-15 23:28:34 [INFO]	[TRAIN] epoch=2, iter=1030/2000, loss=0.0153, lr=0.000300, batch_cost=0.2542, reader_cost=0.00012, ips=15.7384 samples/sec | ETA 00:04:06
    2021-04-15 23:28:37 [INFO]	[TRAIN] epoch=2, iter=1040/2000, loss=0.0165, lr=0.000300, batch_cost=0.2542, reader_cost=0.00014, ips=15.7328 samples/sec | ETA 00:04:04
    2021-04-15 23:28:39 [INFO]	[TRAIN] epoch=2, iter=1050/2000, loss=0.0159, lr=0.000300, batch_cost=0.2570, reader_cost=0.00010, ips=15.5634 samples/sec | ETA 00:04:04
    2021-04-15 23:28:42 [INFO]	[TRAIN] epoch=2, iter=1060/2000, loss=0.0184, lr=0.000300, batch_cost=0.2550, reader_cost=0.00014, ips=15.6885 samples/sec | ETA 00:03:59
    2021-04-15 23:28:44 [INFO]	[TRAIN] epoch=2, iter=1070/2000, loss=0.0156, lr=0.000300, batch_cost=0.2540, reader_cost=0.00012, ips=15.7453 samples/sec | ETA 00:03:56
    2021-04-15 23:28:47 [INFO]	[TRAIN] epoch=2, iter=1080/2000, loss=0.0178, lr=0.000300, batch_cost=0.2565, reader_cost=0.00017, ips=15.5919 samples/sec | ETA 00:03:56
    2021-04-15 23:28:50 [INFO]	[TRAIN] epoch=2, iter=1090/2000, loss=0.0202, lr=0.000300, batch_cost=0.2547, reader_cost=0.00013, ips=15.7068 samples/sec | ETA 00:03:51
    2021-04-15 23:28:52 [INFO]	[TRAIN] epoch=2, iter=1100/2000, loss=0.0167, lr=0.000300, batch_cost=0.2539, reader_cost=0.00011, ips=15.7535 samples/sec | ETA 00:03:48
    2021-04-15 23:28:55 [INFO]	[TRAIN] epoch=2, iter=1110/2000, loss=0.0121, lr=0.000300, batch_cost=0.2540, reader_cost=0.00013, ips=15.7487 samples/sec | ETA 00:03:46
    2021-04-15 23:28:57 [INFO]	[TRAIN] epoch=2, iter=1120/2000, loss=0.0140, lr=0.000300, batch_cost=0.2555, reader_cost=0.00011, ips=15.6569 samples/sec | ETA 00:03:44
    2021-04-15 23:29:00 [INFO]	[TRAIN] epoch=2, iter=1130/2000, loss=0.0128, lr=0.000300, batch_cost=0.2555, reader_cost=0.00012, ips=15.6534 samples/sec | ETA 00:03:42
    2021-04-15 23:29:02 [INFO]	[TRAIN] epoch=2, iter=1140/2000, loss=0.0139, lr=0.000300, batch_cost=0.2542, reader_cost=0.00014, ips=15.7356 samples/sec | ETA 00:03:38
    2021-04-15 23:29:05 [INFO]	[TRAIN] epoch=2, iter=1150/2000, loss=0.0111, lr=0.000300, batch_cost=0.2546, reader_cost=0.00010, ips=15.7083 samples/sec | ETA 00:03:36
    2021-04-15 23:29:07 [INFO]	[TRAIN] epoch=2, iter=1160/2000, loss=0.0137, lr=0.000300, batch_cost=0.2532, reader_cost=0.00014, ips=15.7973 samples/sec | ETA 00:03:32
    2021-04-15 23:29:10 [INFO]	[TRAIN] epoch=2, iter=1170/2000, loss=0.0137, lr=0.000300, batch_cost=0.2537, reader_cost=0.00011, ips=15.7686 samples/sec | ETA 00:03:30
    2021-04-15 23:29:12 [INFO]	[TRAIN] epoch=2, iter=1180/2000, loss=0.0149, lr=0.000300, batch_cost=0.2541, reader_cost=0.00014, ips=15.7442 samples/sec | ETA 00:03:28
    2021-04-15 23:29:15 [INFO]	[TRAIN] epoch=2, iter=1190/2000, loss=0.0149, lr=0.000300, batch_cost=0.2541, reader_cost=0.00011, ips=15.7405 samples/sec | ETA 00:03:25
    2021-04-15 23:29:18 [INFO]	[TRAIN] epoch=2, iter=1200/2000, loss=0.0147, lr=0.000300, batch_cost=0.2552, reader_cost=0.00009, ips=15.6762 samples/sec | ETA 00:03:24
    2021-04-15 23:29:18 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0172 - reader cost: 1.2887e-0


    2021-04-15 23:29:26 [INFO]	[EVAL] #Images=453 mIoU=0.8381 Acc=0.9947 Kappa=0.9577 
    2021-04-15 23:29:26 [INFO]	[EVAL] Class IoU: 
    [0.9952 0.9231 0.5961]
    2021-04-15 23:29:26 [INFO]	[EVAL] Class Acc: 
    [0.998  0.9501 0.9227]
    2021-04-15 23:29:28 [INFO]	[EVAL] The model with the best validation mIoU (0.8381) was saved at iter 1200.
    2021-04-15 23:29:30 [INFO]	[TRAIN] epoch=2, iter=1210/2000, loss=0.0193, lr=0.000300, batch_cost=0.2529, reader_cost=0.00011, ips=15.8192 samples/sec | ETA 00:03:19
    2021-04-15 23:29:33 [INFO]	[TRAIN] epoch=2, iter=1220/2000, loss=0.0142, lr=0.000300, batch_cost=0.2538, reader_cost=0.00010, ips=15.7634 samples/sec | ETA 00:03:17
    2021-04-15 23:29:35 [INFO]	[TRAIN] epoch=2, iter=1230/2000, loss=0.0139, lr=0.000300, batch_cost=0.2541, reader_cost=0.00011, ips=15.7413 samples/sec | ETA 00:03:15
    2021-04-15 23:29:38 [INFO]	[TRAIN] epoch=2, iter=1240/2000, loss=0.0169, lr=0.000300, batch_cost=0.2535, reader_cost=0.00014, ips=15.7791 samples/sec | ETA 00:03:12
    2021-04-15 23:29:40 [INFO]	[TRAIN] epoch=2, iter=1250/2000, loss=0.0158, lr=0.000300, batch_cost=0.2541, reader_cost=0.00015, ips=15.7441 samples/sec | ETA 00:03:10
    2021-04-15 23:29:43 [INFO]	[TRAIN] epoch=2, iter=1260/2000, loss=0.0149, lr=0.000300, batch_cost=0.2548, reader_cost=0.00010, ips=15.7002 samples/sec | ETA 00:03:08
    2021-04-15 23:29:46 [INFO]	[TRAIN] epoch=2, iter=1270/2000, loss=0.0136, lr=0.000300, batch_cost=0.2551, reader_cost=0.00011, ips=15.6828 samples/sec | ETA 00:03:06
    2021-04-15 23:29:48 [INFO]	[TRAIN] epoch=2, iter=1280/2000, loss=0.0161, lr=0.000300, batch_cost=0.2546, reader_cost=0.00011, ips=15.7082 samples/sec | ETA 00:03:03
    2021-04-15 23:29:51 [INFO]	[TRAIN] epoch=2, iter=1290/2000, loss=0.0138, lr=0.000300, batch_cost=0.2544, reader_cost=0.00012, ips=15.7244 samples/sec | ETA 00:03:00
    2021-04-15 23:29:53 [INFO]	[TRAIN] epoch=2, iter=1300/2000, loss=0.0129, lr=0.000300, batch_cost=0.2539, reader_cost=0.00010, ips=15.7554 samples/sec | ETA 00:02:57
    2021-04-15 23:29:56 [INFO]	[TRAIN] epoch=2, iter=1310/2000, loss=0.0137, lr=0.000300, batch_cost=0.2548, reader_cost=0.00013, ips=15.6987 samples/sec | ETA 00:02:55
    2021-04-15 23:29:58 [INFO]	[TRAIN] epoch=2, iter=1320/2000, loss=0.0153, lr=0.000300, batch_cost=0.2551, reader_cost=0.00009, ips=15.6800 samples/sec | ETA 00:02:53
    2021-04-15 23:30:01 [INFO]	[TRAIN] epoch=2, iter=1330/2000, loss=0.0172, lr=0.000300, batch_cost=0.2543, reader_cost=0.00011, ips=15.7281 samples/sec | ETA 00:02:50
    2021-04-15 23:30:03 [INFO]	[TRAIN] epoch=2, iter=1340/2000, loss=0.0179, lr=0.000300, batch_cost=0.2547, reader_cost=0.00012, ips=15.7059 samples/sec | ETA 00:02:48
    2021-04-15 23:30:06 [INFO]	[TRAIN] epoch=2, iter=1350/2000, loss=0.0123, lr=0.000300, batch_cost=0.2543, reader_cost=0.00011, ips=15.7294 samples/sec | ETA 00:02:45
    2021-04-15 23:30:08 [INFO]	[TRAIN] epoch=2, iter=1360/2000, loss=0.0135, lr=0.000300, batch_cost=0.2550, reader_cost=0.00009, ips=15.6882 samples/sec | ETA 00:02:43
    2021-04-15 23:30:11 [INFO]	[TRAIN] epoch=2, iter=1370/2000, loss=0.0119, lr=0.000300, batch_cost=0.2549, reader_cost=0.00012, ips=15.6930 samples/sec | ETA 00:02:40
    2021-04-15 23:30:14 [INFO]	[TRAIN] epoch=2, iter=1380/2000, loss=0.0146, lr=0.000300, batch_cost=0.2562, reader_cost=0.00012, ips=15.6141 samples/sec | ETA 00:02:38
    2021-04-15 23:30:16 [INFO]	[TRAIN] epoch=2, iter=1390/2000, loss=0.0207, lr=0.000300, batch_cost=0.2545, reader_cost=0.00011, ips=15.7140 samples/sec | ETA 00:02:35
    2021-04-15 23:30:19 [INFO]	[TRAIN] epoch=2, iter=1400/2000, loss=0.0242, lr=0.000300, batch_cost=0.2545, reader_cost=0.00010, ips=15.7148 samples/sec | ETA 00:02:32
    2021-04-15 23:30:19 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0171 - reader cost: 1.4981e-0


    2021-04-15 23:30:27 [INFO]	[EVAL] #Images=453 mIoU=0.7496 Acc=0.9912 Kappa=0.9287 
    2021-04-15 23:30:27 [INFO]	[EVAL] Class IoU: 
    [0.9933 0.871  0.3846]
    2021-04-15 23:30:27 [INFO]	[EVAL] Class Acc: 
    [0.9952 0.9655 0.4255]
    2021-04-15 23:30:28 [INFO]	[EVAL] The model with the best validation mIoU (0.8381) was saved at iter 1200.
    2021-04-15 23:30:31 [INFO]	[TRAIN] epoch=2, iter=1410/2000, loss=0.0156, lr=0.000300, batch_cost=0.2534, reader_cost=0.00013, ips=15.7841 samples/sec | ETA 00:02:29
    2021-04-15 23:30:33 [INFO]	[TRAIN] epoch=2, iter=1420/2000, loss=0.0152, lr=0.000300, batch_cost=0.2537, reader_cost=0.00010, ips=15.7664 samples/sec | ETA 00:02:27
    2021-04-15 23:30:36 [INFO]	[TRAIN] epoch=2, iter=1430/2000, loss=0.0166, lr=0.000300, batch_cost=0.2537, reader_cost=0.00014, ips=15.7676 samples/sec | ETA 00:02:24
    2021-04-15 23:30:38 [INFO]	[TRAIN] epoch=2, iter=1440/2000, loss=0.0186, lr=0.000300, batch_cost=0.2543, reader_cost=0.00010, ips=15.7284 samples/sec | ETA 00:02:22
    2021-04-15 23:30:41 [INFO]	[TRAIN] epoch=2, iter=1450/2000, loss=0.0139, lr=0.000300, batch_cost=0.2545, reader_cost=0.00012, ips=15.7180 samples/sec | ETA 00:02:19
    2021-04-15 23:30:43 [INFO]	[TRAIN] epoch=2, iter=1460/2000, loss=0.0137, lr=0.000300, batch_cost=0.2569, reader_cost=0.00009, ips=15.5726 samples/sec | ETA 00:02:18
    2021-04-15 23:30:46 [INFO]	[TRAIN] epoch=2, iter=1470/2000, loss=0.0172, lr=0.000300, batch_cost=0.2543, reader_cost=0.00010, ips=15.7297 samples/sec | ETA 00:02:14
    2021-04-15 23:30:49 [INFO]	[TRAIN] epoch=2, iter=1480/2000, loss=0.0165, lr=0.000300, batch_cost=0.2546, reader_cost=0.00030, ips=15.7100 samples/sec | ETA 00:02:12
    2021-04-15 23:30:51 [INFO]	[TRAIN] epoch=2, iter=1490/2000, loss=0.0128, lr=0.000300, batch_cost=0.2552, reader_cost=0.00015, ips=15.6766 samples/sec | ETA 00:02:10
    2021-04-15 23:30:54 [INFO]	[TRAIN] epoch=2, iter=1500/2000, loss=0.0154, lr=0.000300, batch_cost=0.2540, reader_cost=0.00010, ips=15.7450 samples/sec | ETA 00:02:07
    2021-04-15 23:30:56 [INFO]	[TRAIN] epoch=2, iter=1510/2000, loss=0.0110, lr=0.000300, batch_cost=0.2549, reader_cost=0.00011, ips=15.6911 samples/sec | ETA 00:02:04
    2021-04-15 23:30:59 [INFO]	[TRAIN] epoch=2, iter=1520/2000, loss=0.0126, lr=0.000300, batch_cost=0.2542, reader_cost=0.00012, ips=15.7336 samples/sec | ETA 00:02:02
    2021-04-15 23:31:01 [INFO]	[TRAIN] epoch=2, iter=1530/2000, loss=0.0108, lr=0.000300, batch_cost=0.2546, reader_cost=0.00011, ips=15.7104 samples/sec | ETA 00:01:59
    2021-04-15 23:31:04 [INFO]	[TRAIN] epoch=2, iter=1540/2000, loss=0.0117, lr=0.000300, batch_cost=0.2549, reader_cost=0.00011, ips=15.6917 samples/sec | ETA 00:01:57
    2021-04-15 23:31:06 [INFO]	[TRAIN] epoch=2, iter=1550/2000, loss=0.0129, lr=0.000300, batch_cost=0.2543, reader_cost=0.00009, ips=15.7288 samples/sec | ETA 00:01:54
    2021-04-15 23:31:09 [INFO]	[TRAIN] epoch=2, iter=1560/2000, loss=0.0100, lr=0.000300, batch_cost=0.2589, reader_cost=0.00011, ips=15.4479 samples/sec | ETA 00:01:53
    2021-04-15 23:31:12 [INFO]	[TRAIN] epoch=2, iter=1570/2000, loss=0.0089, lr=0.000300, batch_cost=0.2545, reader_cost=0.00012, ips=15.7149 samples/sec | ETA 00:01:49
    2021-04-15 23:31:14 [INFO]	[TRAIN] epoch=2, iter=1580/2000, loss=0.0109, lr=0.000300, batch_cost=0.2549, reader_cost=0.00010, ips=15.6935 samples/sec | ETA 00:01:47
    2021-04-15 23:31:17 [INFO]	[TRAIN] epoch=2, iter=1590/2000, loss=0.0121, lr=0.000300, batch_cost=0.2548, reader_cost=0.00015, ips=15.6976 samples/sec | ETA 00:01:44
    2021-04-15 23:31:19 [INFO]	[TRAIN] epoch=2, iter=1600/2000, loss=0.0127, lr=0.000300, batch_cost=0.2545, reader_cost=0.00016, ips=15.7188 samples/sec | ETA 00:01:41
    2021-04-15 23:31:19 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0171 - reader cost: 1.2335e-


    2021-04-15 23:31:27 [INFO]	[EVAL] #Images=453 mIoU=0.8801 Acc=0.9950 Kappa=0.9605 
    2021-04-15 23:31:27 [INFO]	[EVAL] Class IoU: 
    [0.9953 0.9275 0.7175]
    2021-04-15 23:31:27 [INFO]	[EVAL] Class Acc: 
    [0.9988 0.9464 0.8465]
    2021-04-15 23:31:29 [INFO]	[EVAL] The model with the best validation mIoU (0.8801) was saved at iter 1600.
    2021-04-15 23:31:32 [INFO]	[TRAIN] epoch=2, iter=1610/2000, loss=0.0131, lr=0.000300, batch_cost=0.2540, reader_cost=0.00014, ips=15.7480 samples/sec | ETA 00:01:39
    2021-04-15 23:31:35 [INFO]	[TRAIN] epoch=2, iter=1620/2000, loss=0.0123, lr=0.000300, batch_cost=0.2548, reader_cost=0.00010, ips=15.6973 samples/sec | ETA 00:01:36
    2021-04-15 23:31:37 [INFO]	[TRAIN] epoch=2, iter=1630/2000, loss=0.0163, lr=0.000300, batch_cost=0.2553, reader_cost=0.00011, ips=15.6704 samples/sec | ETA 00:01:34
    2021-04-15 23:31:40 [INFO]	[TRAIN] epoch=2, iter=1640/2000, loss=0.0117, lr=0.000300, batch_cost=0.2541, reader_cost=0.00009, ips=15.7421 samples/sec | ETA 00:01:31
    2021-04-15 23:31:42 [INFO]	[TRAIN] epoch=2, iter=1650/2000, loss=0.0188, lr=0.000300, batch_cost=0.2545, reader_cost=0.00012, ips=15.7195 samples/sec | ETA 00:01:29
    2021-04-15 23:31:45 [INFO]	[TRAIN] epoch=2, iter=1660/2000, loss=0.0146, lr=0.000300, batch_cost=0.2555, reader_cost=0.00015, ips=15.6565 samples/sec | ETA 00:01:26
    2021-04-15 23:31:47 [INFO]	[TRAIN] epoch=2, iter=1670/2000, loss=0.0184, lr=0.000300, batch_cost=0.2543, reader_cost=0.00010, ips=15.7280 samples/sec | ETA 00:01:23
    2021-04-15 23:31:50 [INFO]	[TRAIN] epoch=2, iter=1680/2000, loss=0.0136, lr=0.000300, batch_cost=0.2540, reader_cost=0.00010, ips=15.7463 samples/sec | ETA 00:01:21
    2021-04-15 23:31:52 [INFO]	[TRAIN] epoch=2, iter=1690/2000, loss=0.0189, lr=0.000300, batch_cost=0.2551, reader_cost=0.00013, ips=15.6825 samples/sec | ETA 00:01:19
    2021-04-15 23:31:55 [INFO]	[TRAIN] epoch=2, iter=1700/2000, loss=0.0172, lr=0.000300, batch_cost=0.2536, reader_cost=0.00011, ips=15.7728 samples/sec | ETA 00:01:16
    2021-04-15 23:31:57 [INFO]	[TRAIN] epoch=2, iter=1710/2000, loss=0.0172, lr=0.000300, batch_cost=0.2541, reader_cost=0.00010, ips=15.7418 samples/sec | ETA 00:01:13
    2021-04-15 23:32:00 [INFO]	[TRAIN] epoch=2, iter=1720/2000, loss=0.0208, lr=0.000300, batch_cost=0.2543, reader_cost=0.00011, ips=15.7324 samples/sec | ETA 00:01:11
    2021-04-15 23:32:03 [INFO]	[TRAIN] epoch=2, iter=1730/2000, loss=0.0157, lr=0.000300, batch_cost=0.2548, reader_cost=0.00011, ips=15.6960 samples/sec | ETA 00:01:08
    2021-04-15 23:32:05 [INFO]	[TRAIN] epoch=2, iter=1740/2000, loss=0.0183, lr=0.000300, batch_cost=0.2541, reader_cost=0.00011, ips=15.7433 samples/sec | ETA 00:01:06
    2021-04-15 23:32:08 [INFO]	[TRAIN] epoch=2, iter=1750/2000, loss=0.0127, lr=0.000300, batch_cost=0.2544, reader_cost=0.00014, ips=15.7220 samples/sec | ETA 00:01:03
    2021-04-15 23:32:10 [INFO]	[TRAIN] epoch=2, iter=1760/2000, loss=0.0152, lr=0.000300, batch_cost=0.2543, reader_cost=0.00009, ips=15.7286 samples/sec | ETA 00:01:01
    2021-04-15 23:32:13 [INFO]	[TRAIN] epoch=2, iter=1770/2000, loss=0.0155, lr=0.000300, batch_cost=0.2542, reader_cost=0.00010, ips=15.7347 samples/sec | ETA 00:00:58
    2021-04-15 23:32:15 [INFO]	[TRAIN] epoch=2, iter=1780/2000, loss=0.0162, lr=0.000300, batch_cost=0.2549, reader_cost=0.00011, ips=15.6908 samples/sec | ETA 00:00:56
    2021-04-15 23:32:18 [INFO]	[TRAIN] epoch=2, iter=1790/2000, loss=0.0166, lr=0.000300, batch_cost=0.2549, reader_cost=0.00010, ips=15.6938 samples/sec | ETA 00:00:53
    2021-04-15 23:32:20 [INFO]	[TRAIN] epoch=2, iter=1800/2000, loss=0.0124, lr=0.000300, batch_cost=0.2544, reader_cost=0.00010, ips=15.7203 samples/sec | ETA 00:00:50
    2021-04-15 23:32:20 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0172 - reader cost: 1.2854e-0


    2021-04-15 23:32:28 [INFO]	[EVAL] #Images=453 mIoU=0.8556 Acc=0.9952 Kappa=0.9615 
    2021-04-15 23:32:28 [INFO]	[EVAL] Class IoU: 
    [0.9956 0.9294 0.6418]
    2021-04-15 23:32:28 [INFO]	[EVAL] Class Acc: 
    [0.9973 0.9666 0.9279]
    2021-04-15 23:32:30 [INFO]	[EVAL] The model with the best validation mIoU (0.8801) was saved at iter 1600.
    2021-04-15 23:32:32 [INFO]	[TRAIN] epoch=3, iter=1810/2000, loss=0.0153, lr=0.000300, batch_cost=0.2602, reader_cost=0.00725, ips=15.3717 samples/sec | ETA 00:00:49
    2021-04-15 23:32:35 [INFO]	[TRAIN] epoch=3, iter=1820/2000, loss=0.0124, lr=0.000300, batch_cost=0.2556, reader_cost=0.00010, ips=15.6470 samples/sec | ETA 00:00:46
    2021-04-15 23:32:38 [INFO]	[TRAIN] epoch=3, iter=1830/2000, loss=0.0236, lr=0.000300, batch_cost=0.2560, reader_cost=0.00012, ips=15.6279 samples/sec | ETA 00:00:43
    2021-04-15 23:32:40 [INFO]	[TRAIN] epoch=3, iter=1840/2000, loss=0.0133, lr=0.000300, batch_cost=0.2545, reader_cost=0.00010, ips=15.7179 samples/sec | ETA 00:00:40
    2021-04-15 23:32:43 [INFO]	[TRAIN] epoch=3, iter=1850/2000, loss=0.0182, lr=0.000300, batch_cost=0.2540, reader_cost=0.00010, ips=15.7469 samples/sec | ETA 00:00:38
    2021-04-15 23:32:45 [INFO]	[TRAIN] epoch=3, iter=1860/2000, loss=0.0117, lr=0.000300, batch_cost=0.2543, reader_cost=0.00010, ips=15.7271 samples/sec | ETA 00:00:35
    2021-04-15 23:32:48 [INFO]	[TRAIN] epoch=3, iter=1870/2000, loss=0.0168, lr=0.000300, batch_cost=0.2540, reader_cost=0.00010, ips=15.7450 samples/sec | ETA 00:00:33
    2021-04-15 23:32:50 [INFO]	[TRAIN] epoch=3, iter=1880/2000, loss=0.0132, lr=0.000300, batch_cost=0.2544, reader_cost=0.00011, ips=15.7217 samples/sec | ETA 00:00:30
    2021-04-15 23:32:53 [INFO]	[TRAIN] epoch=3, iter=1890/2000, loss=0.0124, lr=0.000300, batch_cost=0.2538, reader_cost=0.00010, ips=15.7625 samples/sec | ETA 00:00:27
    2021-04-15 23:32:55 [INFO]	[TRAIN] epoch=3, iter=1900/2000, loss=0.0161, lr=0.000300, batch_cost=0.2550, reader_cost=0.00011, ips=15.6861 samples/sec | ETA 00:00:25
    2021-04-15 23:32:58 [INFO]	[TRAIN] epoch=3, iter=1910/2000, loss=0.0095, lr=0.000300, batch_cost=0.2549, reader_cost=0.00015, ips=15.6925 samples/sec | ETA 00:00:22
    2021-04-15 23:33:01 [INFO]	[TRAIN] epoch=3, iter=1920/2000, loss=0.0137, lr=0.000300, batch_cost=0.2541, reader_cost=0.00015, ips=15.7393 samples/sec | ETA 00:00:20
    2021-04-15 23:33:03 [INFO]	[TRAIN] epoch=3, iter=1930/2000, loss=0.0181, lr=0.000300, batch_cost=0.2552, reader_cost=0.00010, ips=15.6750 samples/sec | ETA 00:00:17
    2021-04-15 23:33:06 [INFO]	[TRAIN] epoch=3, iter=1940/2000, loss=0.0173, lr=0.000300, batch_cost=0.2541, reader_cost=0.00010, ips=15.7444 samples/sec | ETA 00:00:15
    2021-04-15 23:33:08 [INFO]	[TRAIN] epoch=3, iter=1950/2000, loss=0.0142, lr=0.000300, batch_cost=0.2551, reader_cost=0.00011, ips=15.6803 samples/sec | ETA 00:00:12
    2021-04-15 23:33:11 [INFO]	[TRAIN] epoch=3, iter=1960/2000, loss=0.0122, lr=0.000300, batch_cost=0.2539, reader_cost=0.00010, ips=15.7526 samples/sec | ETA 00:00:10
    2021-04-15 23:33:13 [INFO]	[TRAIN] epoch=3, iter=1970/2000, loss=0.0125, lr=0.000300, batch_cost=0.2542, reader_cost=0.00012, ips=15.7354 samples/sec | ETA 00:00:07
    2021-04-15 23:33:16 [INFO]	[TRAIN] epoch=3, iter=1980/2000, loss=0.0124, lr=0.000300, batch_cost=0.2544, reader_cost=0.00009, ips=15.7211 samples/sec | ETA 00:00:05
    2021-04-15 23:33:18 [INFO]	[TRAIN] epoch=3, iter=1990/2000, loss=0.0123, lr=0.000300, batch_cost=0.2543, reader_cost=0.00011, ips=15.7276 samples/sec | ETA 00:00:02
    2021-04-15 23:33:21 [INFO]	[TRAIN] epoch=3, iter=2000/2000, loss=0.0140, lr=0.000300, batch_cost=0.2543, reader_cost=0.00011, ips=15.7298 samples/sec | ETA 00:00:00
    2021-04-15 23:33:21 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


    453/453 [==============================] - 8s 18ms/step - batch_cost: 0.0174 - reader cost: 3.6937e-


    2021-04-15 23:33:29 [INFO]	[EVAL] #Images=453 mIoU=0.8826 Acc=0.9956 Kappa=0.9648 
    2021-04-15 23:33:29 [INFO]	[EVAL] Class IoU: 
    [0.9959 0.9351 0.7168]
    2021-04-15 23:33:29 [INFO]	[EVAL] Class Acc: 
    [0.998  0.9633 0.8962]
    2021-04-15 23:33:31 [INFO]	[EVAL] The model with the best validation mIoU (0.8826) was saved at iter 2000.
    <class 'paddle.nn.layer.conv.Conv2D'>'s flops has been counted
    Customize Function has been applied to <class 'paddle.nn.layer.norm.SyncBatchNorm'>
    Cannot find suitable count function for <class 'paddle.nn.layer.pooling.MaxPool2D'>. Treat it as zero FLOPs.
    Total Flops: 124455223296     Total Params: 13404931


    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.FP32, but right dtype is VarType.INT32, the right dtype will convert to VarType.FP32
      format(lhs_dtype, rhs_dtype, lhs_dtype))


### 8.评估测试集

### #Images=453 mIoU=0.8826 Acc=0.9956 Kappa=0.9648


```python
from paddleseg.core import evaluate
model = UNet(num_classes=3)
#换自己保存的模型文件
model_path = 'save_model/best_model/model.pdparams'
para_state_dict = paddle.load(model_path)
model.set_dict(para_state_dict)
evaluate(model,val_dataset)
```

    2021-04-15 23:34:17 [INFO]	Start evaluating (total_samples=453, total_iters=453)...


      4/453 [..............................] - ETA: 10s - batch_cost: 0.0226 - reader cost: 0.0066453/453 [==============================] - 8s 17ms/step - batch_cost: 0.0171 - reader cost: 1.2341e-0


    2021-04-15 23:34:25 [INFO]	[EVAL] #Images=453 mIoU=0.8826 Acc=0.9956 Kappa=0.9648 
    2021-04-15 23:34:25 [INFO]	[EVAL] Class IoU: 
    [0.9959 0.9351 0.7168]
    2021-04-15 23:34:25 [INFO]	[EVAL] Class Acc: 
    [0.998  0.9633 0.8962]





    (0.8826217, 0.9955867)




```python
from paddleseg.core import predict
transforms = T.Compose([
    T.Resize(target_size=(512, 512)),
    T.Normalize()
])

model = UNet(num_classes=3)
#生成图片列表
image_list = []
with open('work/newdata/test_list.txt' ,'r') as f:
    for line in f.readlines():
        image_list.append(line.split()[0])

predict(
        model,
        #换自己保存的模型文件
        model_path = 'save_model/best_model/model.pdparams',
        transforms=transforms,
        image_list=image_list,
        save_dir='results',
    )
```

    2021-04-15 23:39:11 [INFO]	Loading pretrained model from save_model/best_model/model.pdparams
    2021-04-15 23:39:11 [INFO]	There are 112/112 variables loaded into UNet.
    2021-04-15 23:39:11 [INFO]	Start to predict...


    452/452 [==============================] - 19s 43ms/st


### 9.预览分割结果


```python
num = 6
img_list = random.sample(image_list, num)
pre_path = 'results/pseudo_color_prediction'
plt.figure(figsize=(12,num*4))
index = 1
for i in range(len(img_list)):
    plt.subplot(num,3,index)
    img_origin = cv2.imread(img_list[i],0)
    plt.title('origin')
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img_origin,'gray')

    plt.subplot(num,3,index+1)
    label_path = (img_list[i].replace('origin', 'label')).replace('jpg','png')
    img_label = cv2.imread(label_path,0)
    plt.title('label')
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img_label, 'gray')

    plt.subplot(num,3,index+2)
    predict_path = os.path.join(pre_path, os.path.basename(label_path))
    img_pre = cv2.imread(predict_path,0)
    plt.title('predict')
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img_pre, 'gray')

    index += 3

plt.show()
```


![png](output_24_0.png)

